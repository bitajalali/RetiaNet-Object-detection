{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdZSwuaj8Zqp"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir\n",
        "!pip install pycocotools\n",
        "\n",
        "# Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet50\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from PIL import Image\n",
        "from typing import Any, Callable, Optional, Tuple, List\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Define Custom Dataset Class"
      ],
      "metadata": {
        "id": "qYcY3IiE8oKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip custom dataset\n",
        "!curl -L \"https://public.roboflow.com/ds/6F1fnWqEs1?key=ixsFJhWXgt\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "\n",
        "class CarDetection(torch.utils.data.Dataset):\n",
        "    def __init__(self, root: str, annFile: str, transform: Optional[Callable] = None):\n",
        "        from pycocotools.coco import COCO\n",
        "        self.root = root\n",
        "        self.coco = COCO(annFile)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.transforms = transform\n",
        "\n",
        "    def _load_image(self, id: int) -> Image.Image:\n",
        "        path = self.coco.loadImgs(id)[0][\"file_name\"]\n",
        "        return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
        "\n",
        "    def _load_target(self, id) -> List[Any]:\n",
        "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        id = self.ids[index]\n",
        "        image = self._load_image(id)\n",
        "        target = self._load_target(id)\n",
        "        targets = {'boxes': torch.tensor([item['bbox'] for item in target], dtype=torch.float32),\n",
        "                   'labels': torch.tensor([item['category_id'] for item in target], dtype=torch.int64)}\n",
        "\n",
        "        if self.transforms:\n",
        "            image, targets = self.transforms(image, targets)\n",
        "\n",
        "        image = torchvision.transforms.functional.to_tensor(image)\n",
        "        return image, targets\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ids)\n",
        "\n",
        "# DataLoader Setup\n",
        "def get_loader(root, json, batch_size=1, shuffle=False, num_workers=4):\n",
        "    dataset = CarDetection(root=root, annFile=json)\n",
        "    data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "    return data_loader\n",
        "\n",
        "train_loader = get_loader('train', '/content/train/_annotations.coco.json', batch_size=2, shuffle=True)\n",
        "val_loader = get_loader('valid', '/content/valid/_annotations.coco.json', batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "tg2Elevz8pQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Custom RetinaNet with FPN"
      ],
      "metadata": {
        "id": "2IJ2MRFx882Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.retinanet import RetinaNet\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Define Custom RetinaNet with FPN\n",
        "class CustomRetinaNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomRetinaNet, self).__init__()\n",
        "        # Define a ResNet-50 backbone with FPN\n",
        "        self.backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
        "        # Create RetinaNet with the FPN backbone\n",
        "        self.retina_net = RetinaNet(backbone=self.backbone, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        return self.retina_net(images, targets)\n",
        "\n",
        "# Initialize Model\n",
        "num_classes = len(train_loader.dataset.coco.getCatIds()) + 1  # Including background class\n",
        "model = CustomRetinaNet(num_classes).to(device)\n"
      ],
      "metadata": {
        "id": "LU1Rkxkx813i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Training Setup"
      ],
      "metadata": {
        "id": "tAzrxSoo9Oov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Training Setup\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    for images, targets in train_loader:\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {losses.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "JjybeH1x9NrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Prediction and Visualization"
      ],
      "metadata": {
        "id": "l64xE3Dq9sNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Visualization Function\n",
        "def visualize_bbox(img, bbox, class_name, score, color=(255, 0, 0), thickness=2):\n",
        "    x_min, y_min, w, h = bbox\n",
        "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
        "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
        "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
        "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), color, -1)\n",
        "    cv2.putText(img, f\"{class_name}: {score:.2f}\", (x_min, y_min - int(0.3 * text_height)), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)\n",
        "    return img\n",
        "\n",
        "# Prediction and Visualization\n",
        "def predict_and_visualize(image_path):\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = torchvision.transforms.functional.to_tensor(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(img_tensor)\n",
        "\n",
        "    img_np = img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    plt.imshow(img_np)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    for box, label, score in zip(predictions[0]['boxes'], predictions[0]['labels'], predictions[0]['scores']):\n",
        "        if score > 0.6:\n",
        "            x_min, y_min, x_max, y_max = box.cpu().numpy()\n",
        "            ax.add_patch(Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, edgecolor=(1, 0, 0), linewidth=2))\n",
        "            ax.text(x_min, y_min, f\"{category_id_to_name[label.item()]}: {score:.2f}\", bbox=dict(facecolor='yellow', alpha=0.5), fontsize=8, color='black')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Test the prediction visualization with a sample image\n",
        "predict_and_visualize('/content/test/sample_image.jpg')  # Replace with an actual path from your test dataset\n"
      ],
      "metadata": {
        "id": "gvozwQIp-AzA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}